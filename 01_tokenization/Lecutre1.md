- GPT

- Transformers are just next words prediction (Predict next words)

## Input sequence and tokens

## Step 1: Tokenization (it is not randomly it is decided by the model)

[tiktokenizer]

- vocab
- vocab Size (GPT 4o 200k)

## Step 2: Vector Embeddings

    Vector Embedding means in human language like cat make cat image in our memory and dog make dog image in our memory.

    Vector Embedding in ai gives semantic meaning to our tokens.

    Vector Embedding are 3D in real life

- Hey, Chai Code
- Token (google/gemma-7b)

  - 2, 6750, 235269, 122233, 6698

- vector embedding visualizer google

## Step 3: Positional Encoding

- Add information about the position of the token in the sentence.

### Self-Attention

- We allow matrixes to talk to each other.

### Multi-Head Attention
